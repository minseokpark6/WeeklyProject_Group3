{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ciw96\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ciw96\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from matplotlib import font_manager, rc\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import chardet\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# nltk 데이터 다운로드 (첫 실행 시 필요)\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# nltk 데이터 다운로드 (첫 실행 시 필요)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 글꼴 경로 지정\n",
    "font_path = \"c:/Windows/Fonts/malgun.ttf\"  # 윈도우에 설치된 맑은 고딕 폰트 경로\n",
    "\n",
    "# 폰트 이름 얻어오기\n",
    "font_name = font_manager.FontProperties(fname=font_path).get_name()\n",
    "\n",
    "# matplotlib의 rc(run command) 기능을 이용하여 글꼴 설정\n",
    "mpl.rc('font', family=font_name)\n",
    "\n",
    "# 유니코드에서  음수 부호 설정\n",
    "mpl.rc('axes', unicode_minus=False)\n",
    "\n",
    "raw = pd.read_csv('../../../../../datasets/paris_reviews.csv')\n",
    "df = raw.copy()\n",
    "\n",
    "df = df[['listing_id','date','comments']]\n",
    "\n",
    "df = df.rename(columns= {\n",
    "    'listing_id' : '숙소_id',\n",
    "    'date': '리뷰날짜',\n",
    "    'comments' : '리뷰'\n",
    "})\n",
    "\n",
    "# 리뷰 결측치 제거\n",
    "df = df[~df['리뷰'].isnull()]\n",
    "\n",
    "# 리뷰 내의 <br> 제거\n",
    "df['리뷰'] = df['리뷰'].str.replace('<br>\\s*', ' ', regex=True)\n",
    "df['리뷰'] = df['리뷰'].str.replace('<br/>\\s*', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1793899"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ciw96\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ciw96\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk 데이터 다운로드 (첫 실행 시 필요)\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# nltk 데이터 다운로드 (첫 실행 시 필요)\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 감성 분석 (2시간 10분 걸림)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감성 분석 함수 정의\n",
    "def analyze_sentiment_vader(comment):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    scores = sid.polarity_scores(comment)\n",
    "    return scores\n",
    "\n",
    "def analyze_sentiment_textblob(comment):\n",
    "    analysis = TextBlob(comment)\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "# VADER 감성 분석 결과 추가\n",
    "df['vader_sentiment'] = df['리뷰'].apply(lambda x: analyze_sentiment_vader(str(x)))\n",
    "\n",
    "# TextBlob 감성 분석 결과 추가\n",
    "df['textblob_sentiment'] = df['리뷰'].apply(lambda x: analyze_sentiment_textblob(str(x)))\n",
    "\n",
    "# VADER 점수 세부 항목 분리\n",
    "df['부정'] = df['vader_sentiment'].apply(lambda x: x['neg'])  # 부정적인 감성의 비율\n",
    "df['중립'] = df['vader_sentiment'].apply(lambda x: x['neu'])  # 중립적인 감성의 비율\n",
    "df['긍정'] = df['vader_sentiment'].apply(lambda x: x['pos'])  # 긍정적인 감성의 비율\n",
    "df['전체'] = df['vader_sentiment'].apply(lambda x: x['compound'])  # 전체적인 감성 점수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>숙소_id</th>\n",
       "      <th>리뷰날짜</th>\n",
       "      <th>리뷰</th>\n",
       "      <th>vader_sentiment</th>\n",
       "      <th>textblob_sentiment</th>\n",
       "      <th>부정</th>\n",
       "      <th>중립</th>\n",
       "      <th>긍정</th>\n",
       "      <th>전체</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39948</td>\n",
       "      <td>2013-09-20</td>\n",
       "      <td>Aliyah et Philippe m'ont réservé un accueil ex...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39948</td>\n",
       "      <td>2013-09-29</td>\n",
       "      <td>Aliyah and Philippe are gracious hosts and eve...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.69, 'pos': 0.31, 'compou...</td>\n",
       "      <td>0.325417</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.9847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3109</td>\n",
       "      <td>2017-10-28</td>\n",
       "      <td>Tout s'est bien déroulé. Merci bien. PG</td>\n",
       "      <td>{'neg': 0.2, 'neu': 0.8, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3109</td>\n",
       "      <td>2017-11-03</td>\n",
       "      <td>Un petit nid fouiller douillet situé dans  app...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3109</td>\n",
       "      <td>2018-07-24</td>\n",
       "      <td>Appartement spacieux, propre,clair, et calme à...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794001</th>\n",
       "      <td>1107872133955829208</td>\n",
       "      <td>2024-03-16</td>\n",
       "      <td>We had the best time staying at this apartment...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.641, 'pos': 0.359, 'comp...</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.9854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794002</th>\n",
       "      <td>1109396868279302811</td>\n",
       "      <td>2024-03-15</td>\n",
       "      <td>On a passé un magnifique séjour dans l'apparte...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.964, 'pos': 0.036, 'comp...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.6784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794003</th>\n",
       "      <td>1108741370485532713</td>\n",
       "      <td>2024-03-10</td>\n",
       "      <td>Superbe découverte que cet appartement d Hugo ...</td>\n",
       "      <td>{'neg': 0.025, 'neu': 0.937, 'pos': 0.038, 'co...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794004</th>\n",
       "      <td>1109220943409848089</td>\n",
       "      <td>2024-03-14</td>\n",
       "      <td>El apartamento es mejor que en las fotos. Todo...</td>\n",
       "      <td>{'neg': 0.049, 'neu': 0.951, 'pos': 0.0, 'comp...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.5983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794005</th>\n",
       "      <td>1110936505905289590</td>\n",
       "      <td>2024-03-17</td>\n",
       "      <td>Merci à Fabienne pour sa réactivité et son acc...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1793899 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       숙소_id        리뷰날짜  \\\n",
       "0                      39948  2013-09-20   \n",
       "1                      39948  2013-09-29   \n",
       "2                       3109  2017-10-28   \n",
       "3                       3109  2017-11-03   \n",
       "4                       3109  2018-07-24   \n",
       "...                      ...         ...   \n",
       "1794001  1107872133955829208  2024-03-16   \n",
       "1794002  1109396868279302811  2024-03-15   \n",
       "1794003  1108741370485532713  2024-03-10   \n",
       "1794004  1109220943409848089  2024-03-14   \n",
       "1794005  1110936505905289590  2024-03-17   \n",
       "\n",
       "                                                        리뷰  \\\n",
       "0        Aliyah et Philippe m'ont réservé un accueil ex...   \n",
       "1        Aliyah and Philippe are gracious hosts and eve...   \n",
       "2                  Tout s'est bien déroulé. Merci bien. PG   \n",
       "3        Un petit nid fouiller douillet situé dans  app...   \n",
       "4        Appartement spacieux, propre,clair, et calme à...   \n",
       "...                                                    ...   \n",
       "1794001  We had the best time staying at this apartment...   \n",
       "1794002  On a passé un magnifique séjour dans l'apparte...   \n",
       "1794003  Superbe découverte que cet appartement d Hugo ...   \n",
       "1794004  El apartamento es mejor que en las fotos. Todo...   \n",
       "1794005  Merci à Fabienne pour sa réactivité et son acc...   \n",
       "\n",
       "                                           vader_sentiment  \\\n",
       "0        {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "1        {'neg': 0.0, 'neu': 0.69, 'pos': 0.31, 'compou...   \n",
       "2        {'neg': 0.2, 'neu': 0.8, 'pos': 0.0, 'compound...   \n",
       "3        {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "4        {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "...                                                    ...   \n",
       "1794001  {'neg': 0.0, 'neu': 0.641, 'pos': 0.359, 'comp...   \n",
       "1794002  {'neg': 0.0, 'neu': 0.964, 'pos': 0.036, 'comp...   \n",
       "1794003  {'neg': 0.025, 'neu': 0.937, 'pos': 0.038, 'co...   \n",
       "1794004  {'neg': 0.049, 'neu': 0.951, 'pos': 0.0, 'comp...   \n",
       "1794005  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "\n",
       "         textblob_sentiment     부정     중립     긍정      전체  \n",
       "0                  0.000000  0.000  1.000  0.000  0.0000  \n",
       "1                  0.325417  0.000  0.690  0.310  0.9847  \n",
       "2                  0.000000  0.200  0.800  0.000 -0.1280  \n",
       "3                  0.000000  0.000  1.000  0.000  0.0000  \n",
       "4                  0.000000  0.000  1.000  0.000  0.0000  \n",
       "...                     ...    ...    ...    ...     ...  \n",
       "1794001            0.575000  0.000  0.641  0.359  0.9854  \n",
       "1794002            0.500000  0.000  0.964  0.036  0.6784  \n",
       "1794003            0.333333  0.025  0.937  0.038  0.3400  \n",
       "1794004            0.000000  0.049  0.951  0.000 -0.5983  \n",
       "1794005            0.000000  0.000  1.000  0.000  0.0000  \n",
       "\n",
       "[1793899 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../../../../datasets/paris_reviews_check.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../../../datasets/paris_reviews_check.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>숙소_id</th>\n",
       "      <th>리뷰날짜</th>\n",
       "      <th>리뷰</th>\n",
       "      <th>vader_sentiment</th>\n",
       "      <th>textblob_sentiment</th>\n",
       "      <th>부정</th>\n",
       "      <th>중립</th>\n",
       "      <th>긍정</th>\n",
       "      <th>전체</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1793894</th>\n",
       "      <td>1107872133955829208</td>\n",
       "      <td>2024-03-16</td>\n",
       "      <td>We had the best time staying at this apartment...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.641, 'pos': 0.359, 'comp...</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.9854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793895</th>\n",
       "      <td>1109396868279302811</td>\n",
       "      <td>2024-03-15</td>\n",
       "      <td>On a passé un magnifique séjour dans l'apparte...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.964, 'pos': 0.036, 'comp...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.6784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793896</th>\n",
       "      <td>1108741370485532713</td>\n",
       "      <td>2024-03-10</td>\n",
       "      <td>Superbe découverte que cet appartement d Hugo ...</td>\n",
       "      <td>{'neg': 0.025, 'neu': 0.937, 'pos': 0.038, 'co...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793897</th>\n",
       "      <td>1109220943409848089</td>\n",
       "      <td>2024-03-14</td>\n",
       "      <td>El apartamento es mejor que en las fotos. Todo...</td>\n",
       "      <td>{'neg': 0.049, 'neu': 0.951, 'pos': 0.0, 'comp...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.5983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793898</th>\n",
       "      <td>1110936505905289590</td>\n",
       "      <td>2024-03-17</td>\n",
       "      <td>Merci à Fabienne pour sa réactivité et son acc...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       숙소_id        리뷰날짜  \\\n",
       "1793894  1107872133955829208  2024-03-16   \n",
       "1793895  1109396868279302811  2024-03-15   \n",
       "1793896  1108741370485532713  2024-03-10   \n",
       "1793897  1109220943409848089  2024-03-14   \n",
       "1793898  1110936505905289590  2024-03-17   \n",
       "\n",
       "                                                        리뷰  \\\n",
       "1793894  We had the best time staying at this apartment...   \n",
       "1793895  On a passé un magnifique séjour dans l'apparte...   \n",
       "1793896  Superbe découverte que cet appartement d Hugo ...   \n",
       "1793897  El apartamento es mejor que en las fotos. Todo...   \n",
       "1793898  Merci à Fabienne pour sa réactivité et son acc...   \n",
       "\n",
       "                                           vader_sentiment  \\\n",
       "1793894  {'neg': 0.0, 'neu': 0.641, 'pos': 0.359, 'comp...   \n",
       "1793895  {'neg': 0.0, 'neu': 0.964, 'pos': 0.036, 'comp...   \n",
       "1793896  {'neg': 0.025, 'neu': 0.937, 'pos': 0.038, 'co...   \n",
       "1793897  {'neg': 0.049, 'neu': 0.951, 'pos': 0.0, 'comp...   \n",
       "1793898  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "\n",
       "         textblob_sentiment     부정     중립     긍정      전체  \n",
       "1793894            0.575000  0.000  0.641  0.359  0.9854  \n",
       "1793895            0.500000  0.000  0.964  0.036  0.6784  \n",
       "1793896            0.333333  0.025  0.937  0.038  0.3400  \n",
       "1793897            0.000000  0.049  0.951  0.000 -0.5983  \n",
       "1793898            0.000000  0.000  1.000  0.000  0.0000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰 확인 결과 <br/>도 있음\n",
    "\n",
    "df['리뷰'] = df['리뷰'].str.replace('<br/>\\s*', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in c:\\users\\ciw96\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\ciw96\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 부정개수가 너무 적음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m코드를 실행할 수 없습니다. 세션이 삭제되었습니다. 커널을 다시 시작해 보세요."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m코드를 실행할 수 없습니다. 세션이 삭제되었습니다. 커널을 다시 시작해 보세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;''''''''''''''''''''''''''''''''''''''''''''''\n",
    "from langdetect import detect\n",
    "\n",
    "initial_sample = df.sample(frac=0.5, random_state=1)\n",
    "\n",
    "# 영어 리뷰 필터링 함수\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# 영어 리뷰 필터링\n",
    "initial_sample['is_english'] = initial_sample['리뷰'].apply(is_english)\n",
    "english_reviews = initial_sample[initial_sample['is_english']]\n",
    "\n",
    "# 최종적으로 20,000개의 영어 리뷰 추출\n",
    "final_sample = english_reviews.sample(n=20000, random_state=1)\n",
    "';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;''''''''''''''''''''''''''''''''''''''''''''''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 걍 1:1로 가져옴\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 프레임 샘플링\n",
    "initial_sample = df.sample(frac=0.5, random_state=1)\n",
    "\n",
    "# 영어 리뷰 필터링 함수\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# 영어 리뷰 필터링\n",
    "initial_sample['is_english'] = initial_sample['리뷰'].apply(is_english)\n",
    "english_reviews = initial_sample[initial_sample['is_english']]\n",
    "\n",
    "# 긍정적 리뷰와 부정적 리뷰 필터링\n",
    "positive_reviews = english_reviews[english_reviews['전체'] > 0.05]\n",
    "negative_reviews = english_reviews[english_reviews['전체'] < -0.05]\n",
    "\n",
    "# 긍정적 리뷰와 부정적 리뷰에서 각각 10,000개씩 샘플링\n",
    "final_positive_sample = positive_reviews.sample(n=10000, random_state=1) if len(positive_reviews) >= 10000 else positive_reviews\n",
    "final_negative_sample = negative_reviews.sample(n=10000, random_state=1) if len(negative_reviews) >= 10000 else negative_reviews\n",
    "\n",
    "# 최종 샘플 합치기\n",
    "final_sample = pd.concat([final_positive_sample, final_negative_sample])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종적으로 20,000개의 영어 리뷰 추출\n",
    "final_sample = english_reviews.sample(n=20000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sample.to_csv('../../../../../datasets/paris_reviews_check1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf\n",
    "- 키워드 추출\n",
    "- 여기서 쓸모없을거 같은 단어는 뺌(이야기 해봐야함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ciw96\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정적인 리뷰에서 상위 20개 단어:\n",
      "           word    tfidf_score\n",
      "0     apartment  116880.747758\n",
      "49     location  113100.446788\n",
      "42         host   69664.281016\n",
      "13        clean   63570.423613\n",
      "56        metro   58853.850318\n",
      "72    recommend   54789.232810\n",
      "31   everything   53845.795139\n",
      "29           et   51895.904817\n",
      "14        close   48446.178943\n",
      "97         well   47364.431676\n",
      "71       really   44491.280117\n",
      "16  comfortable   43722.629237\n",
      "25         easy   43680.996130\n",
      "74  restaurants   41031.118572\n",
      "53       lovely   40673.534020\n",
      "92         très   40214.869860\n",
      "40      helpful   39586.866581\n",
      "11        check   36769.535565\n",
      "37         flat   36302.215789\n",
      "23   definitely   35495.756779\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# nltk 데이터 다운로드 (첫 실행 시 필요)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 긍정적 리뷰와 부정적 리뷰로 분류\n",
    "positive_reviews = df[df['전체'] > 0.05]['리뷰'].dropna().astype(str)\n",
    "negative_reviews = df[df['전체'] < -0.05]['리뷰'].dropna().astype(str)\n",
    "\n",
    "# 영어, 스페인어, 독일어 불용어 목록 결합 및 리스트로 변환\n",
    "stop_words = list(set(stopwords.words('english')) | set(stopwords.words('spanish')) | set(stopwords.words('german')))\n",
    "\n",
    "# 추가로 제외할 단어 목록\n",
    "additional_stop_words = [ 'paris','super','perfect','place', 'stay', 'chicago', 'airbnb', 'would', 'us','great','nice','good','amazing','highly']\n",
    "\n",
    "# 추가적인 불용어 목록을 결합\n",
    "stop_words.extend(additional_stop_words)\n",
    "\n",
    "# TF-IDF 벡터라이저 초기화 (결합된 불용어 목록 사용)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100, stop_words=stop_words)\n",
    "\n",
    "# 긍정적 리뷰에 대한 TF-IDF 행렬 계산\n",
    "tfidf_matrix_positive = tfidf_vectorizer.fit_transform(positive_reviews)\n",
    "feature_names_positive = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_scores_positive = tfidf_matrix_positive.sum(axis=0).A1\n",
    "tfidf_df_positive = pd.DataFrame({'word': feature_names_positive, 'tfidf_score': tfidf_scores_positive})\n",
    "top_tfidf_positive = tfidf_df_positive.nlargest(20, 'tfidf_score')  # 상위 20개 단어\n",
    "\n",
    "# 부정적 리뷰에 대한 TF-IDF 행렬 계산 (불용어 목록 조정)\n",
    "try:\n",
    "    tfidf_matrix_negative = tfidf_vectorizer.fit_transform(negative_reviews)\n",
    "    feature_names_negative = tfidf_vectorizer.get_feature_names_out()\n",
    "    tfidf_scores_negative = tfidf_matrix_negative.sum(axis=0).A1\n",
    "    tfidf_df_negative = pd.DataFrame({'word': feature_names_negative, 'tfidf_score': tfidf_scores_negative})\n",
    "    top_tfidf_negative = tfidf_df_negative.nlargest(20, 'tfidf_score')  # 상위 20개 단어\n",
    "\n",
    "    # 결과 출력\n",
    "    print(\"긍정적인 리뷰에서 상위 20개 단어:\")\n",
    "    print(top_tfidf_positive)\n",
    "except ValueError as e:\n",
    "    print(f\"TF-IDF 계산 중 오류 발생: {e}\")\n",
    "    print(\"부정적 리뷰의 예시:\")\n",
    "    print(negative_reviews.head(10))  # 부정적 리뷰 예시 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1181043\n",
      "147967\n"
     ]
    }
   ],
   "source": [
    "print(len(positive_reviews))\n",
    "print(len(negative_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "부정적인 리뷰에서 상위 20개 단어:\n",
      "             word  tfidf_score\n",
      "20           dire   137.316942\n",
      "73           rien   136.355694\n",
      "63        parfait    91.158704\n",
      "86           tout    84.784115\n",
      "40            gut    58.951216\n",
      "93        wohnung    47.803201\n",
      "79          schön    43.335559\n",
      "85            top    41.924254\n",
      "51           lage    40.481642\n",
      "64        perfekt    35.724667\n",
      "15        comment    30.000000\n",
      "89     unterkunft    27.744252\n",
      "76         sauber    26.481748\n",
      "17     complaints    25.977347\n",
      "6             bad    25.357507\n",
      "10           bien    21.362360\n",
      "99          était    20.668609\n",
      "49  kommunikation    18.422894\n",
      "19          danke    17.432585\n",
      "3      aufenthalt    17.344727\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n부정적인 리뷰에서 상위 20개 단어:\")\n",
    "print(top_tfidf_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lda 분석\n",
    "- 키워드 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([feature_names[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m topic\u001b[38;5;241m.\u001b[39margsort()[:\u001b[38;5;241m-\u001b[39mn_top_words \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(message)\n\u001b[1;32m---> 19\u001b[0m print_top_words(lda, \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names\u001b[49m(), \u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 텍스트 데이터 벡터화\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X = vectorizer.fit_transform(df['리뷰'])\n",
    "\n",
    "# LDA 모델 생성 및 학습\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=0)\n",
    "lda.fit(X)\n",
    "\n",
    "# 주제와 각 주제의 가장 중요한 단어들 출력\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "\n",
    "print_top_words(lda, vectorizer.get_feature_names(), 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mdf\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1          Aliyah and Philippe are gracious hosts and eve...\n",
       "6          L'accueil de Philippe et d'Alihah est toujours...\n",
       "8          I stayed at Aliyah and Philippe's while I was ...\n",
       "9          Absolutely delightful room - clean, roomy, tas...\n",
       "11         I will give a big Thank you to Philippe and hi...\n",
       "                                 ...                        \n",
       "1793889    Nous avons passé un très bon séjour chez Cleme...\n",
       "1793892    L’appartement est parfait, très propre, lumine...\n",
       "1793894    We had the best time staying at this apartment...\n",
       "1793895    On a passé un magnifique séjour dans l'apparte...\n",
       "1793896    Superbe découverte que cet appartement d Hugo ...\n",
       "Name: 리뷰, Length: 1181043, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ciw96\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\ciw96\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\ciw96\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ciw96\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.5.10)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ciw96\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ciw96\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ciw96\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('good', 'place'), ('place', 'bad'), ('bad', 'place')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ciw96\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 필요한 nltk 패키지 다운로드\n",
    "nltk.download('punkt')  # 토큰화에 필요한 데이터\n",
    "nltk.download('stopwords')  # 불용어 데이터\n",
    "\n",
    "# 이제 다시 토큰화 시도\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def generate_bigrams(text):\n",
    "    words = word_tokenize(text.lower())  # 소문자 변환 후 토큰화\n",
    "    stop_words = set(stopwords.words('english'))  # 영어 불용어 목록\n",
    "    filtered_words = [word for word in words if word not in stop_words and word.isalpha()]  # 불용어 제거 및 숫자, 특수 문자 제거\n",
    "    bigram_list = list(bigrams(filtered_words))  # bigram 생성\n",
    "    return bigram_list\n",
    "\n",
    "example_text = \"It was a good place but not a bad place.\"\n",
    "bigrams = generate_bigrams(example_text)\n",
    "print(bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
